{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch as th\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "from braindecode.models.deep4 import Deep4Net\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "from braindecode.datasets.bcic_iv_2a import BCICompetition4Set2A\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, \\\n",
    "    RuntimeMonitor, CroppedTrialMisclassMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.datautil.splitters import split_into_two_sets\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "from braindecode.torch_ext.util import set_random_seeds, np_to_var\n",
    "from braindecode.mne_ext.signalproc import mne_apply\n",
    "from braindecode.datautil.signalproc import (bandpass_cnt,\n",
    "                                             exponential_running_standardize)\n",
    "from braindecode.datautil.trial_segment import create_signal_target_from_raw_mne\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Shallow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s %(levelname)s : %(message)s',\n",
    "                    level=logging.DEBUG, stream=sys.stdout)\n",
    "# Should contain both .gdf files and .mat-labelfiles from competition\n",
    "data_folder = '/home/david/data/BCICIV_2a_gdf/'\n",
    "subject_id = 1 # 1-9\n",
    "low_cut_hz = 4 # 0 or 4\n",
    "model = 'shallow' #'shallow' or 'deep'\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/david/data/BCICIV_2a_gdf/A01T.gdf...\n",
      "GDF file detected\n",
      "Overlapping events detected. Use find_edf_events for the original events.\n",
      "Setting channel info structure...\n",
      "Interpolating stim channel. Events may jitter.\n",
      "Creating raw.info structure...\n",
      "Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "Reading 0 ... 672527  =      0.000 ...  2690.108 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/mne/io/edf/edf.py:1028: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Overlapping events detected. Use find_edf_events for the original events.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Interpolating stim channel. Events may jitter.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/david/data/BCICIV_2a_gdf/A01E.gdf...\n",
      "GDF file detected\n",
      "Overlapping events detected. Use find_edf_events for the original events.\n",
      "Setting channel info structure...\n",
      "Interpolating stim channel. Events may jitter.\n",
      "Creating raw.info structure...\n",
      "Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "Reading 0 ... 686999  =      0.000 ...  2747.996 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/mne/io/edf/edf.py:1028: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, np.uint8).tolist()[0]\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Overlapping events detected. Use find_edf_events for the original events.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Interpolating stim channel. Events may jitter.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n",
      "/home/david/anaconda3/envs/machinelearning/lib/python3.6/site-packages/braindecode/datasets/bcic_iv_2a.py:20: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  raw_edf = mne.io.read_raw_edf(self.filename, stim_channel='auto')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-01 19:52:06,144 INFO : Trial per class:\n",
      "Counter({'Tongue': 72, 'Foot': 72, 'Right Hand': 72, 'Left Hand': 72})\n",
      "2018-10-01 19:52:06,377 INFO : Trial per class:\n",
      "Counter({'Left Hand': 72, 'Right Hand': 72, 'Foot': 72, 'Tongue': 72})\n"
     ]
    }
   ],
   "source": [
    "ival = [-500, 4000]\n",
    "input_time_length = 1125\n",
    "max_epochs = 800\n",
    "max_increase_epochs = 80\n",
    "batch_size = 60\n",
    "high_cut_hz = 38\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "valid_set_fraction = 0.2\n",
    "\n",
    "train_filename = 'A{:02d}T.gdf'.format(subject_id)\n",
    "test_filename = 'A{:02d}E.gdf'.format(subject_id)\n",
    "train_filepath = os.path.join(data_folder, train_filename)\n",
    "test_filepath = os.path.join(data_folder, test_filename)\n",
    "train_label_filepath = train_filepath.replace('.gdf', '.mat')\n",
    "test_label_filepath = test_filepath.replace('.gdf', '.mat')\n",
    "\n",
    "train_loader = BCICompetition4Set2A(\n",
    "    train_filepath, labels_filename=train_label_filepath)\n",
    "test_loader = BCICompetition4Set2A(\n",
    "    test_filepath, labels_filename=test_label_filepath)\n",
    "train_cnt = train_loader.load()\n",
    "test_cnt = test_loader.load()\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "train_cnt = train_cnt.drop_channels(['STI 014', 'EOG-left',\n",
    "                                     'EOG-central', 'EOG-right'])\n",
    "assert len(train_cnt.ch_names) == 22\n",
    "# lets convert to millvolt for numerical stability of next operations\n",
    "train_cnt = mne_apply(lambda a: a * 1e6, train_cnt)\n",
    "train_cnt = mne_apply(\n",
    "    lambda a: bandpass_cnt(a, low_cut_hz, high_cut_hz, train_cnt.info['sfreq'],\n",
    "                           filt_order=3,\n",
    "                           axis=1), train_cnt)\n",
    "train_cnt = mne_apply(\n",
    "    lambda a: exponential_running_standardize(a.T, factor_new=factor_new,\n",
    "                                              init_block_size=init_block_size,\n",
    "                                              eps=1e-4).T,\n",
    "    train_cnt)\n",
    "\n",
    "test_cnt = test_cnt.drop_channels(['STI 014', 'EOG-left',\n",
    "                                   'EOG-central', 'EOG-right'])\n",
    "assert len(test_cnt.ch_names) == 22\n",
    "test_cnt = mne_apply(lambda a: a * 1e6, test_cnt)\n",
    "test_cnt = mne_apply(\n",
    "    lambda a: bandpass_cnt(a, low_cut_hz, high_cut_hz, test_cnt.info['sfreq'],\n",
    "                           filt_order=3,\n",
    "                           axis=1), test_cnt)\n",
    "test_cnt = mne_apply(\n",
    "    lambda a: exponential_running_standardize(a.T, factor_new=factor_new,\n",
    "                                              init_block_size=init_block_size,\n",
    "                                              eps=1e-4).T,\n",
    "    test_cnt)\n",
    "\n",
    "marker_def = OrderedDict([('Left Hand', [1]), ('Right Hand', [2],),\n",
    "                          ('Foot', [3]), ('Tongue', [4])])\n",
    "\n",
    "train_set = create_signal_target_from_raw_mne(train_cnt, marker_def, ival)\n",
    "test_set = create_signal_target_from_raw_mne(test_cnt, marker_def, ival)\n",
    "\n",
    "train_set, valid_set = split_into_two_sets(\n",
    "    train_set, first_set_fraction=1-valid_set_fraction)\n",
    "\n",
    "set_random_seeds(seed=20190706, cuda=cuda)\n",
    "\n",
    "n_classes = 4\n",
    "n_chans = int(train_set.X.shape[1])\n",
    "if model == 'shallow':\n",
    "    model = ShallowFBCSPNet(n_chans, n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length=30).create_network()\n",
    "elif model == 'deep':\n",
    "    model = Deep4Net(n_chans, n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length=2).create_network()\n",
    "\n",
    "\n",
    "to_dense_prediction_model(model)\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 22, 1125)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 22, 1125)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-01 19:52:08,153 INFO : Model: \n",
      "Sequential(\n",
      "  (dimshuffle): Expression(expression=_transpose_time_to_spat)\n",
      "  (conv_time): Conv2d(1, 40, kernel_size=(25, 1), stride=(1, 1))\n",
      "  (conv_spat): Conv2d(40, 40, kernel_size=(1, 22), stride=(1, 1), bias=False)\n",
      "  (bnorm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_nonlin): Expression(expression=square)\n",
      "  (pool): AvgPool2d(kernel_size=(75, 1), stride=(1, 1), padding=0)\n",
      "  (pool_nonlin): Expression(expression=safe_log)\n",
      "  (drop): Dropout(p=0.5)\n",
      "  (conv_classifier): Conv2d(40, 4, kernel_size=(30, 1), stride=(1, 1), dilation=(15, 1))\n",
      "  (softmax): LogSoftmax()\n",
      "  (squeeze): Expression(expression=_squeeze_final_output)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "log.info(\"Model: \\n{:s}\".format(str(model)))\n",
    "dummy_input = np_to_var(train_set.X[:1, :, :, None])\n",
    "if cuda:\n",
    "    dummy_input = dummy_input.cuda()\n",
    "out = model(dummy_input)\n",
    "\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "iterator = CropsFromTrialsIterator(batch_size=batch_size,\n",
    "                                   input_time_length=input_time_length,\n",
    "                                   n_preds_per_input=n_preds_per_input)\n",
    "\n",
    "stop_criterion = Or([MaxEpochs(max_epochs),\n",
    "                     NoDecrease('valid_misclass', max_increase_epochs)])\n",
    "\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(\n",
    "                input_time_length=input_time_length), RuntimeMonitor()]\n",
    "\n",
    "model_constraint = MaxNormDefaultConstraint()\n",
    "\n",
    "loss_function = lambda preds, targets: F.nll_loss(\n",
    "    th.mean(preds, dim=2, keepdim=False), targets)\n",
    "\n",
    "exp = Experiment(model, train_set, valid_set, test_set, iterator=iterator,\n",
    "                 loss_function=loss_function, optimizer=optimizer,\n",
    "                 model_constraint=model_constraint,\n",
    "                 monitors=monitors,\n",
    "                 stop_criterion=stop_criterion,\n",
    "                 remember_best_column='valid_misclass',\n",
    "                 run_after_early_stop=True, cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test feature_vars shape\n",
      "(288, 22, 1125)\n"
     ]
    }
   ],
   "source": [
    "print(\"test feature_vars shape\")\n",
    "print(exp.datasets['test'].X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test target_vars\n",
      "[0 1 1 0 1 0 1 2 1 3 0 2 1 0 3 3 3 3 3 0 2 1 0 0 2 3 0 2 2 2 0 1 0 1 1 0 1\n",
      " 2 1 2 2 3 2 2 3 3 3 3 3 2 1 0 0 1 2 3 1 2 0 0 0 3 1 1 0 0 2 0 1 3 3 2 0 3\n",
      " 3 1 3 3 1 0 1 2 2 2 3 2 0 3 1 2 1 2 3 1 2 0 0 0 3 1 0 2 0 2 1 3 0 2 2 0 2\n",
      " 1 3 3 3 2 0 3 1 3 1 0 2 1 0 2 2 0 2 3 3 1 0 1 3 1 3 2 1 1 1 2 3 0 1 3 0 2\n",
      " 2 3 0 0 2 1 3 3 3 1 0 2 1 3 0 3 2 1 3 3 0 1 1 2 3 1 0 0 3 1 0 2 1 1 2 0 3\n",
      " 2 2 2 2 0 1 0 1 0 0 2 2 1 2 3 0 3 0 0 1 3 2 1 3 2 3 2 3 1 1 3 0 1 1 1 2 3\n",
      " 0 3 0 2 0 3 0 2 0 1 2 2 3 0 1 3 1 2 2 0 3 1 3 0 0 2 2 1 3 1 1 0 1 3 3 1 1\n",
      " 1 1 3 3 2 3 0 1 2 1 0 3 0 3 0 0 0 0 2 2 3 1 2 2 2 3 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"test target_vars\")\n",
    "print(exp.datasets['test'].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test target_vars shape: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"test target_vars shape:\", exp.datasets['test'].y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 60\n",
      "input_time_length: 1125\n",
      "n_preds_per_input: 592\n"
     ]
    }
   ],
   "source": [
    "print(\"batch_size:\", exp.iterator.batch_size)\n",
    "print(\"input_time_length:\", exp.iterator.input_time_length)\n",
    "print(\"n_preds_per_input:\", exp.iterator.n_preds_per_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_receptive_field: 534\n"
     ]
    }
   ],
   "source": [
    "n_receptive_field = exp.iterator.input_time_length - exp.iterator.n_preds_per_input + 1\n",
    "i_trial_starts = [n_receptive_field - 1] * len(exp.datasets['test'].X)\n",
    "i_trial_stops = [trial.shape[1] for trial in exp.datasets['test'].X]\n",
    "\n",
    "print(\"n_receptive_field:\", n_receptive_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_vars shape: (58, 22, 1125, 1)\n",
      "target_vars shape: (58,)\n",
      "feature_vars shape: (58, 22, 1125, 1)\n",
      "target_vars shape: (58,)\n",
      "feature_vars shape: (58, 22, 1125, 1)\n",
      "target_vars shape: (58,)\n",
      "feature_vars shape: (57, 22, 1125, 1)\n",
      "target_vars shape: (57,)\n",
      "feature_vars shape: (57, 22, 1125, 1)\n",
      "target_vars shape: (57,)\n"
     ]
    }
   ],
   "source": [
    "all_actual = []\n",
    "dataset = 'test'\n",
    "for batch in exp.iterator.get_batches(exp.datasets[dataset], shuffle=False):\n",
    "\n",
    "    print(\"feature_vars shape:\", np.array(batch[0]).shape)\n",
    "    print(\"target_vars shape:\", np.array(batch[1]).shape)\n",
    "    \n",
    "    feature_vars = np_to_var(batch[0], pin_memory = exp.pin_memory)\n",
    "    target_vars = np_to_var(batch[1], pin_memory = exp.pin_memory)\n",
    "\n",
    "    if exp.cuda:\n",
    "        feature_vars = feature_vars.cuda()\n",
    "        target_vars = target_vars.cuda()\n",
    "\n",
    "    if hasattr(target_vars, 'cpu'):\n",
    "        target_vars = target_vars.cpu().data.numpy()\n",
    "    else:\n",
    "        # assume it is iterable\n",
    "        target_vars = [o.cpu().data.numpy() for o in target_vars]\n",
    "    \n",
    "    all_actual += target_vars.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 1, 2, 1, 3, 0, 2, 1, 0, 3, 3, 3, 3, 3, 0, 2, 1,\n",
       "       0, 0, 2, 3, 0, 2, 2, 2, 0, 1, 0, 1, 1, 0, 1, 2, 1, 2, 2, 3, 2, 2,\n",
       "       3, 3, 3, 3, 3, 2, 1, 0, 0, 1, 2, 3, 1, 2, 0, 0, 0, 3, 1, 1, 0, 0,\n",
       "       2, 0, 1, 3, 3, 2, 0, 3, 3, 1, 3, 3, 1, 0, 1, 2, 2, 2, 3, 2, 0, 3,\n",
       "       1, 2, 1, 2, 3, 1, 2, 0, 0, 0, 3, 1, 0, 2, 0, 2, 1, 3, 0, 2, 2, 0,\n",
       "       2, 1, 3, 3, 3, 2, 0, 3, 1, 3, 1, 0, 2, 1, 0, 2, 2, 0, 2, 3, 3, 1,\n",
       "       0, 1, 3, 1, 3, 2, 1, 1, 1, 2, 3, 0, 1, 3, 0, 2, 2, 3, 0, 0, 2, 1,\n",
       "       3, 3, 3, 1, 0, 2, 1, 3, 0, 3, 2, 1, 3, 3, 0, 1, 1, 2, 3, 1, 0, 0,\n",
       "       3, 1, 0, 2, 1, 1, 2, 0, 3, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1,\n",
       "       2, 3, 0, 3, 0, 0, 1, 3, 2, 1, 3, 2, 3, 2, 3, 1, 1, 3, 0, 1, 1, 1,\n",
       "       2, 3, 0, 3, 0, 2, 0, 3, 0, 2, 0, 1, 2, 2, 3, 0, 1, 3, 1, 2, 2, 0,\n",
       "       3, 1, 3, 0, 0, 2, 2, 1, 3, 1, 1, 0, 1, 3, 3, 1, 1, 1, 1, 3, 3, 2,\n",
       "       3, 0, 1, 2, 1, 0, 3, 0, 3, 0, 0, 0, 0, 2, 2, 3, 1, 2, 2, 2, 3, 2,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_actual).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-01 19:52:09,110 INFO : Time only for training updates: 0.11s\n",
      "2018-10-01 19:52:09,284 INFO : Epoch 0\n",
      "2018-10-01 19:52:09,284 INFO : train_loss                6.16400\n",
      "2018-10-01 19:52:09,284 INFO : valid_loss                6.76005\n",
      "2018-10-01 19:52:09,285 INFO : test_loss                 6.17513\n",
      "2018-10-01 19:52:09,285 INFO : train_sample_misclass     0.74288\n",
      "2018-10-01 19:52:09,285 INFO : valid_sample_misclass     0.77633\n",
      "2018-10-01 19:52:09,286 INFO : test_sample_misclass      0.74974\n",
      "2018-10-01 19:52:09,286 INFO : train_misclass            0.74348\n",
      "2018-10-01 19:52:09,286 INFO : valid_misclass            0.77586\n",
      "2018-10-01 19:52:09,287 INFO : test_misclass             0.75000\n",
      "2018-10-01 19:52:09,287 INFO : runtime                   0.00000\n",
      "2018-10-01 19:52:09,287 INFO : \n"
     ]
    }
   ],
   "source": [
    "exp.setup_training()\n",
    "exp.run_one_epoch(exp.datasets, remember_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape\n",
      "(58, 22, 1125, 1)\n",
      "targets\n",
      "[3 2 1 0 0 1 2 3 1 2 0 0 0 3 1 1 0 0 2 0 1 3 3 2 0 3 3 1 3 3 1 0 1 2 2 2 3\n",
      " 2 0 3 1 2 1 2 3 1 2 0 0 0 3 1 0 2 0 2 1 3]\n",
      "==========\n",
      "inputs shape\n",
      "(58, 22, 1125, 1)\n",
      "targets\n",
      "[0 2 2 0 2 1 3 3 3 2 0 3 1 3 1 0 2 1 0 2 2 0 2 3 3 1 0 1 3 1 3 2 1 1 1 2 3\n",
      " 0 1 3 0 2 2 3 0 0 2 1 3 3 3 1 0 2 1 3 0 3]\n",
      "==========\n",
      "inputs shape\n",
      "(57, 22, 1125, 1)\n",
      "targets\n",
      "[2 1 3 3 0 1 1 2 3 1 0 0 3 1 0 2 1 1 2 0 3 2 2 2 2 0 1 0 1 0 0 2 2 1 2 3 0\n",
      " 3 0 0 1 3 2 1 3 2 3 2 3 1 1 3 0 1 1 1 2]\n",
      "==========\n",
      "inputs shape\n",
      "(57, 22, 1125, 1)\n",
      "targets\n",
      "[3 0 3 0 2 0 3 0 2 0 1 2 2 3 0 1 3 1 2 2 0 3 1 3 0 0 2 2 1 3 1 1 0 1 3 3 1\n",
      " 1 1 1 3 3 2 3 0 1 2 1 0 3 0 3 0 0 0 0 2]\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "batch_generator = exp.iterator.get_batches(exp.datasets['train'],\n",
    "                                            shuffle=False)\n",
    "start_train_epoch_time = time.time()\n",
    "\n",
    "all_targets = []\n",
    "for inputs, targets in batch_generator:\n",
    "    if exp.batch_modifier is not None:\n",
    "        inputs, targets = exp.batch_modifier.process(inputs,\n",
    "                                                      targets)\n",
    "    if len(inputs) > 0:\n",
    "        all_targets += targets.tolist()\n",
    "    print(\"inputs shape\")\n",
    "    print(inputs.shape)\n",
    "    print(\"targets\")\n",
    "    print(targets)\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0, 0, 1, 2, 3, 1, 2, 0, 0, 0, 3, 1, 1, 0, 0, 2, 0, 1, 3,\n",
       "       3, 2, 0, 3, 3, 1, 3, 3, 1, 0, 1, 2, 2, 2, 3, 2, 0, 3, 1, 2, 1, 2,\n",
       "       3, 1, 2, 0, 0, 0, 3, 1, 0, 2, 0, 2, 1, 3, 0, 2, 2, 0, 2, 1, 3, 3,\n",
       "       3, 2, 0, 3, 1, 3, 1, 0, 2, 1, 0, 2, 2, 0, 2, 3, 3, 1, 0, 1, 3, 1,\n",
       "       3, 2, 1, 1, 1, 2, 3, 0, 1, 3, 0, 2, 2, 3, 0, 0, 2, 1, 3, 3, 3, 1,\n",
       "       0, 2, 1, 3, 0, 3, 2, 1, 3, 3, 0, 1, 1, 2, 3, 1, 0, 0, 3, 1, 0, 2,\n",
       "       1, 1, 2, 0, 3, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 2, 3, 0, 3,\n",
       "       0, 0, 1, 3, 2, 1, 3, 2, 3, 2, 3, 1, 1, 3, 0, 1, 1, 1, 2, 3, 0, 3,\n",
       "       0, 2, 0, 3, 0, 2, 0, 1, 2, 2, 3, 0, 1, 3, 1, 2, 2, 0, 3, 1, 3, 0,\n",
       "       0, 2, 2, 1, 3, 1, 1, 0, 1, 3, 3, 1, 1, 1, 1, 3, 3, 2, 3, 0, 1, 2,\n",
       "       1, 0, 3, 0, 3, 0, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_targets).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 0, 0, 1, 2, 3, 1, 2, 0, 0, 0, 3, 1, 1, 0, 0, 2, 0, 1, 3,\n",
       "       3, 2, 0, 3, 3, 1, 3, 3, 1, 0, 1, 2, 2, 2, 3, 2, 0, 3, 1, 2, 1, 2,\n",
       "       3, 1, 2, 0, 0, 0, 3, 1, 0, 2, 0, 2, 1, 3, 0, 2, 2, 0, 2, 1, 3, 3,\n",
       "       3, 2, 0, 3, 1, 3, 1, 0, 2, 1, 0, 2, 2, 0, 2, 3, 3, 1, 0, 1, 3, 1,\n",
       "       3, 2, 1, 1, 1, 2, 3, 0, 1, 3, 0, 2, 2, 3, 0, 0, 2, 1, 3, 3, 3, 1,\n",
       "       0, 2, 1, 3, 0, 3, 2, 1, 3, 3, 0, 1, 1, 2, 3, 1, 0, 0, 3, 1, 0, 2,\n",
       "       1, 1, 2, 0, 3, 2, 2, 2, 2, 0, 1, 0, 1, 0, 0, 2, 2, 1, 2, 3, 0, 3,\n",
       "       0, 0, 1, 3, 2, 1, 3, 2, 3, 2, 3, 1, 1, 3, 0, 1, 1, 1, 2, 3, 0, 3,\n",
       "       0, 2, 0, 3, 0, 2, 0, 1, 2, 2, 3, 0, 1, 3, 1, 2, 2, 0, 3, 1, 3, 0,\n",
       "       0, 2, 2, 1, 3, 1, 1, 0, 1, 3, 3, 1, 1, 1, 1, 3, 3, 2, 3, 0, 1, 2,\n",
       "       1, 0, 3, 0, 3, 0, 0, 0, 0, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.datasets['train'].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.datasets['train'].y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
